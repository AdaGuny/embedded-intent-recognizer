{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92c4b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import numpy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c95eccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a modell class which is extendable \n",
    "# (e.g. further classification using different models)\n",
    "# currently takes one arguments which is the pre-trained weights\n",
    "# an optimization task would be using the weights from a fine tuned distillBERT model\n",
    "\n",
    "# Some theory\n",
    "# we can use any probabilistic classifier (multinomial logistic regression, k-nearest neighbours etc.)\n",
    "# research show that for real world scenarios active learning is usefull\n",
    "# AL is especially usefull when labeling data is costly\n",
    "# it can also combat imbalanced data-sets (highly dense class A)\n",
    "# it is also usefull in multiclass classification tasks\n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, pre_trained_weights):\n",
    "        \n",
    "        # initialize tokenizer, distillBERT model, binarizer and multioutput random forest classifier\n",
    "        self.tokenizer = ppb.AutoTokenizer.from_pretrained(pre_trained_weights)\n",
    "        self.model = ppb.DistilBertModel.from_pretrained(pre_trained_weights)\n",
    "        self.binarizer = MultiLabelBinarizer()\n",
    "        self.rf = MultiOutputClassifier(RandomForestClassifier())\n",
    "                                                    \n",
    "        # hard coded training data and labels\n",
    "        # further implementation step would be reading data from a file (csv, json etc.)\n",
    "        self.X= ['What is the weather like today?',\n",
    "         'Tell me the weather?',\n",
    "         'What is the weather like in Paris today?',\n",
    "         'Tell me an interesting fact.',\n",
    "         'Tell me a fact.',\n",
    "         'What is the weather like in Berlin today?',\n",
    "         'What is the weather like in Istanbul today?']\n",
    "                                                    \n",
    "        self.y=  [['Weather'], \n",
    "              ['Weather'], \n",
    "              ['Weather', 'City'], \n",
    "              ['Fact'], \n",
    "              ['Fact'],  \n",
    "              ['Weather', 'City'], \n",
    "              ['Weather', 'City']]\n",
    "        \n",
    "    # function to train multioutput random forest classifier                                            \n",
    "    def train_rf(self):\n",
    "                                 \n",
    "        # tokenized embbeddings to send pre-trained model\n",
    "        # they have to be padded so that they have the same length as tensors\n",
    "        # tokenizer deals with data cleaning such as special characters\n",
    "        encoded_input = self.tokenizer(self.X, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # running tokenized embeddings through distillBERT\n",
    "        # attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = self.model(encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])\n",
    "\n",
    "        # save classification tokens for each hidden unit (768) from the last output layer of the distillBERT model\n",
    "        # saved cls tokens are called features\n",
    "        features = last_hidden_states[0][:,0,:].numpy()\n",
    "        \n",
    "        # create a numpy array from labels\n",
    "        y = numpy.array(self.y, dtype=object)\n",
    "    \n",
    "        # binarize the labels\n",
    "        b_y = self.binarizer.fit_transform(y)\n",
    "        \n",
    "        # train random forest classifier using cls tokens and labels \n",
    "        self.rf.fit(features, b_y)\n",
    "        \n",
    "        return \n",
    "            \n",
    "    \n",
    "    # function to find the intent of a given sentence\n",
    "    # intent/sentiment analysis is a part of text classification for nlp tasks\n",
    "    def find_intent(self, input_text):\n",
    "        \n",
    "        # tokenize user input                                            \n",
    "        encoded_input = self.tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # run user input through distillBERT model\n",
    "        with torch.no_grad():\n",
    "            last_hidden_states = self.model(encoded_input['input_ids'], attention_mask=encoded_input['attention_mask'])    \n",
    "\n",
    "        # save features\n",
    "        features = last_hidden_states[0][:,0,:].numpy()\n",
    "    \n",
    "        # save prediction of user input in the form of binary values\n",
    "        prediction = self.rf.predict(features)[0]\n",
    "    \n",
    "        # get the actual label from binarizer\n",
    "        intent = list(self.binarizer.inverse_transform(prediction.reshape(1, -1)))\n",
    "\n",
    "        # return intent\n",
    "        return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09dcf6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('City', 'Weather')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initilize model using pre-trained weights\n",
    "model = Model('distilbert-base-cased')\n",
    "# train random forest\n",
    "model.train_rf()\n",
    "#Find intent\n",
    "model.find_intent(\"What is the weather like in Paris today?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
